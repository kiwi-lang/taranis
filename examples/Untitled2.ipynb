{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbd6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taranis.core.server.server import Server\n",
    "from taranis.core.server.client import Client\n",
    "\n",
    "client = Client(\"http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4c5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b38c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from functools import cache\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from altair import datum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "session_group = client.new_group('PythonPrimer')\n",
    "epoch_tracker = defaultdict(int)\n",
    "\n",
    "\n",
    "def add_metric(run, metric, value):\n",
    "    real_epoch = epoch_tracker[run]\n",
    "    run.new_metric(metric, value, time=real_epoch)\n",
    "            \n",
    "@cache\n",
    "def register_model(name, model):\n",
    "    representation = repr(model)\n",
    "\n",
    "    sha = hashlib.sha256()\n",
    "    sha.update(representation.encode('utf-8'))\n",
    "    digest = sha.hexdigest()\n",
    "    \n",
    "    group = client.get_group(name, meta=dict(digest=digest))\n",
    "    run = client.new_run(name)\n",
    "    group.add_run(run)\n",
    "    session_group.add_run(run)\n",
    "    return run\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81323304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import taranis.core.dataset.split as split\n",
    "from taranis.core.trainer.train import mnist_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets.MNIST(\n",
    "    '../data', \n",
    "    train=True, \n",
    "    download=True\n",
    ")\n",
    "print(len(dataset))\n",
    "\n",
    "def newdataloader(batch_size=512):\n",
    "    # Takes the original dataset and apply transform on the fly\n",
    "    # Convert image to a tensor\n",
    "    # normalize the tensor\n",
    "    dataset_to_use = split.TransformedDatasetClassification(\n",
    "        dataset, \n",
    "        transform=transforms.Compose([                    \n",
    "            transforms.ToTensor(),                         # Transform the image to tensor\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),    # Normalize the image\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Takes care of spliting the dataset into bite size for our model\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset_to_use,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 1,\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdae4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST('../data', train=False,  transform=transform)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=4096, num_workers=1) \n",
    "    \n",
    "    total = len(dataset)\n",
    "    model = model.cpu()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        \n",
    "        for batch, labels in loader:\n",
    "            output = model(batch)\n",
    "            test_loss += F.nll_loss(output, labels, reduction='sum').item()  \n",
    "            count += batch.shape[0]\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "        return test_loss / count, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268590fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, model, epoch=2, lr=0.4):\n",
    "    batch_size = 512\n",
    "    dataloader = newdataloader(batch_size)\n",
    "    \n",
    "    model = model.cpu()\n",
    "    \n",
    "    run = register_model(name, model)\n",
    "    \n",
    "    # Gradient optimizer \n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    \n",
    "    with tqdm(total=len(dataset) * epoch, ncols=100) as progress:\n",
    "\n",
    "        # Repeat a few times \n",
    "        for i in range(epoch):\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "\n",
    "            # Go through the dataset\n",
    "            for batch, labels in dataloader:\n",
    "    \n",
    "                optimizer.zero_grad()                     # Clear previous derivative\n",
    "\n",
    "                probabilities = model(batch)              # Make prediction\n",
    "\n",
    "                loss = F.nll_loss(probabilities, labels)  # Grade the prediction \n",
    "\n",
    "                loss.backward()                           # Compute derivative\n",
    "\n",
    "                optimizer.step()                          # Update parameters using derivative\n",
    "\n",
    "                total_loss += loss.item()                 # Keep track of loss to make sure it goes down\n",
    "\n",
    "                count += 1\n",
    "                \n",
    "                progress.update(batch_size)\n",
    "\n",
    "            loss = total_loss / count\n",
    "            test_loss, acc = test_model(model)\n",
    "            \n",
    "            epoch_tracker[run] += 1\n",
    "            add_metric(run, 'train_loss', loss)\n",
    "            add_metric(run, 'test_loss', test_loss)\n",
    "            add_metric(run, 'test_acc', acc)\n",
    "            \n",
    "            progress.set_description(f\"loss: {loss:8.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7058284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "@cache\n",
    "def newdataloader_cuda(batch_size=512, train=True):\n",
    "    dataset_to_use = mnist_dataset(train)\n",
    "    dataset_to_use.tensors = list(dataset_to_use.tensors)\n",
    "    for i, t in enumerate(dataset_to_use.tensors):\n",
    "        dataset_to_use.tensors[i] = t.cuda()\n",
    "\n",
    "    # Takes care of spliting the dataset into bite size for our model\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset_to_use,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 0,\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def gpu_train(name, original_model, epoch=2, lr=0.4, batch_size=4096):\n",
    "    dataloader = newdataloader_cuda(batch_size)        # HERE optimized loader\n",
    "    device = torch.cuda.current_device()               # HERE Get GPU device\n",
    "    model = original_model.to(device)                  # HERE Convert model to GPU\n",
    "    \n",
    "    run = register_model(name, model)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "\n",
    "    with tqdm(total=len(dataset) * epoch, ncols=100) as progress:\n",
    "        for i in range(epoch):\n",
    "            partial_losses = []\n",
    "            count = 0\n",
    "            for batch, labels in dataloader:\n",
    "                batch, labels = batch.to(device), labels.to(device) # HERE: Convert input to GPU\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                probabilities = model(batch)\n",
    "\n",
    "                loss = F.nll_loss(probabilities, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                partial_losses.append(loss.detach())\n",
    "                count += 1\n",
    "                \n",
    "                progress.update(batch_size)\n",
    "\n",
    "            total_loss = (sum(partial_losses) / count).item()\n",
    "            test_loss, acc = gpu_test_model(model)\n",
    "            \n",
    "            epoch_tracker[run] += 1\n",
    "            add_metric(run, 'train_loss', total_loss)\n",
    "            add_metric(run, 'test_loss', test_loss)\n",
    "            add_metric(run, 'test_acc', acc)\n",
    "            \n",
    "            progress.set_description(f\"loss: {total_loss:8.4f}\")\n",
    "    \n",
    "def gpu_test_model(model, batch_size=4096*2):\n",
    "    dataloader = newdataloader_cuda(batch_size, train=False)     # HERE optimized loader\n",
    "    total = len(dataloader.dataset)\n",
    "    \n",
    "    device = torch.cuda.current_device()  # HERE\n",
    "    model = model.to(device=device)       # HERE\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        for batch, labels in dataloader:\n",
    "            batch, labels = batch.to(device), labels.to(device) # HERE\n",
    "            \n",
    "            output = model(batch)\n",
    "            test_loss += F.nll_loss(output, labels, reduction='sum').item() \n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            total_count += batch.shape[0]\n",
    "\n",
    "        assert total_count == total, f\"{total_count} != {total}\"\n",
    "        return test_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea14c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainfast(*args, **kwargs):\n",
    "    if torch.cuda.is_available():\n",
    "        return gpu_train(*args, **kwargs)\n",
    "    return train(*args, **kwargs)\n",
    "\n",
    "def testfast(*args, **kwargs):\n",
    "    if torch.cuda.is_available():\n",
    "        return gpu_test_model(*args, **kwargs)\n",
    "    return test_model(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09607494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_test_curve():\n",
    "    data = pd.DataFrame(session_group.fetch_metrics())\n",
    "    \n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .encode(\n",
    "            x=alt.X('time'), \n",
    "            y=alt.Y('value', scale=alt.Scale(domain=[-1, -0.7])),\n",
    "            color=alt.Color(\n",
    "                'metric', \n",
    "                legend=alt.Legend(\n",
    "                orient='none',\n",
    "                legendX=130, legendY=-40,\n",
    "                direction='horizontal',\n",
    "                titleAnchor='middle')\n",
    "            )\n",
    "        ).transform_filter(\n",
    "            (datum.metric == 'test_loss') | (datum.metric == 'train_loss')\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ce447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_curve(x, y, c):\n",
    "    data = pd.DataFrame(session_group.fetch_metrics())\n",
    "    \n",
    "    series = data[data['metric'] == y]\n",
    "    mn = series[\"value\"].min()\n",
    "    mx = series[\"value\"].max()\n",
    "    \n",
    "    return (\n",
    "        alt.Chart(series)\n",
    "        .mark_line()\n",
    "        .encode(\n",
    "            x=alt.X(x), \n",
    "            y=alt.Y('value', scale=alt.Scale(domain=[mn, mx])), \n",
    "            color=alt.Color(\n",
    "                c, \n",
    "                legend=alt.Legend(\n",
    "                orient='none',\n",
    "                legendX=130, legendY=-40,\n",
    "                direction='horizontal',\n",
    "                titleAnchor='middle')\n",
    "            )\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26470f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STN, self).__init__()\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 3 * 3, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        \n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        \n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9cc3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:  -0.9252: : 6144000it [01:09, 88919.07it/s]                                                   \n",
      "loss:  -0.9818: : 6144000it [01:05, 93365.37it/s]                                                   \n",
      "loss:  -0.9928: : 6144000it [17:16, 5925.71it/s]                                                    \n",
      "loss:  -0.9892: : 6144000it [01:08, 90349.64it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:04, 94575.20it/s]                                                   \n",
      "loss:  -0.9828: : 6144000it [01:05, 94387.73it/s]                                                   \n",
      "loss:  -0.9930: : 6144000it [17:33, 5832.20it/s]                                                    \n",
      "loss:  -0.9873: : 6144000it [01:09, 88841.01it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 91894.11it/s]                                                   \n",
      "loss:  -0.9824: : 6144000it [01:08, 89499.66it/s]                                                   \n",
      "loss:  -0.9946: : 6144000it [17:14, 5941.70it/s]                                                    \n",
      "loss:  -0.9817: : 6144000it [01:09, 88758.36it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 92930.71it/s]                                                   \n",
      "loss:  -0.8944: : 6144000it [01:07, 90939.27it/s]                                                   \n",
      "loss:  -0.9948: : 6144000it [17:14, 5940.40it/s]                                                    \n",
      "loss:  -0.9834: : 6144000it [01:09, 88930.36it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 92122.22it/s]                                                   \n",
      "loss:  -0.9819: : 6144000it [01:06, 92210.32it/s]                                                   \n",
      "loss:  -0.9955: : 6144000it [17:13, 5945.77it/s]                                                    \n",
      "loss:  -0.9808: : 6144000it [01:11, 86071.69it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:06, 92339.60it/s]                                                   \n",
      "loss:  -0.9828: : 6144000it [01:06, 92013.54it/s]                                                   \n",
      "loss:  -0.9936: : 6144000it [17:13, 5945.49it/s]                                                    \n",
      "loss:  -0.8929: : 6144000it [01:10, 87676.92it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:06, 92267.12it/s]                                                   \n",
      "loss:  -0.8884: : 6144000it [01:06, 92113.44it/s]                                                   \n",
      "loss:  -0.9939: : 6144000it [17:17, 5921.13it/s]                                                    \n",
      "loss:  -0.8873: : 6144000it [01:10, 87632.24it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93948.64it/s]                                                   \n",
      "loss:  -0.9829: : 6144000it [01:06, 92929.68it/s]                                                   \n",
      "loss:  -0.9941: : 6144000it [17:07, 5980.84it/s]                                                    \n",
      "loss:  -0.9808: : 6144000it [01:09, 88372.24it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 92123.80it/s]                                                   \n",
      "loss:  -0.9834: : 6144000it [01:07, 91252.82it/s]                                                   \n",
      "loss:  -0.9950: : 6144000it [17:07, 5978.20it/s]                                                    \n",
      "loss:  -0.9854: : 6144000it [01:08, 89952.43it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93298.90it/s]                                                   \n",
      "loss:  -0.9814: : 6144000it [01:05, 93389.82it/s]                                                   \n",
      "loss:  -0.9922: : 6144000it [17:07, 5979.72it/s]                                                    \n",
      "loss:  -0.9802: : 6144000it [01:08, 89609.35it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93492.08it/s]                                                   \n",
      "loss:  -0.9824: : 6144000it [01:06, 92657.91it/s]                                                   \n",
      "loss:  -0.9950: : 6144000it [17:07, 5978.25it/s]                                                    \n",
      "loss:  -0.9893: : 6144000it [01:08, 90111.30it/s]                                                   \n",
      "loss:  -0.8439: : 6144000it [01:07, 91121.80it/s]                                                   \n",
      "loss:  -0.9816: : 6144000it [01:06, 92275.21it/s]                                                   \n",
      "loss:  -0.8940: : 6144000it [17:08, 5976.34it/s]                                                    \n",
      "loss:  -0.9885: : 6144000it [01:08, 89754.57it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 94298.92it/s]                                                   \n",
      "loss:  -0.9816: : 6144000it [01:05, 93107.55it/s]                                                   \n",
      "loss:  -0.9925: : 6144000it [17:07, 5977.35it/s]                                                    \n",
      "loss:  -0.9867: : 6144000it [01:08, 89792.96it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93356.72it/s]                                                   \n",
      "loss:  -0.9827: : 6144000it [01:05, 93344.22it/s]                                                   \n",
      "loss:  -0.9955: : 6144000it [17:28, 5861.49it/s]                                                    \n",
      "loss:  -0.8877: : 6144000it [01:10, 87327.04it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 92907.75it/s]                                                   \n",
      "loss:  -0.9819: : 6144000it [01:06, 92526.42it/s]                                                   \n",
      "loss:  -0.9040: : 6144000it [17:09, 5967.92it/s]                                                    \n",
      "loss:  -0.9817: : 6144000it [01:08, 89145.74it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93235.91it/s]                                                   \n",
      "loss:  -0.9829: : 6144000it [01:06, 92860.81it/s]                                                   \n",
      "loss:  -0.9863: : 6144000it [17:11, 5955.51it/s]                                                    \n",
      "loss:  -0.9771: : 6144000it [01:09, 89031.25it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93870.26it/s]                                                   \n",
      "loss:  -0.9822: : 6144000it [01:06, 92969.05it/s]                                                   \n",
      "loss:  -0.9042: : 6144000it [17:09, 5968.82it/s]                                                    \n",
      "loss:  -0.9835: : 6144000it [01:09, 88736.93it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93547.89it/s]                                                   \n",
      "loss:  -0.9799: : 6144000it [01:06, 92892.05it/s]                                                   \n",
      "loss:  -0.9935: : 6144000it [22:25, 4566.48it/s]                                                    \n",
      "loss:  -0.9861: : 6144000it [01:08, 89778.40it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93505.20it/s]                                                   \n",
      "loss:  -0.9837: : 6144000it [01:06, 93043.80it/s]                                                   \n",
      "loss:  -0.9957: : 6144000it [22:25, 4565.54it/s]                                                    \n",
      "loss:  -0.9878: : 6144000it [01:08, 89739.58it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93490.14it/s]                                                   \n",
      "loss:  -0.9829: : 6144000it [01:08, 89605.79it/s]                                                   \n",
      "loss:  -0.9947: : 6144000it [22:26, 4563.08it/s]                                                    \n",
      "loss:  -0.9911: : 6144000it [01:08, 89280.85it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:06, 93001.50it/s]                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:  -0.9812: : 6144000it [01:06, 92673.06it/s]                                                   \n",
      "loss:  -0.8971: : 6144000it [22:26, 4563.76it/s]                                                    \n",
      "loss:  -0.9861: : 6144000it [01:08, 89806.61it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93603.47it/s]                                                   \n",
      "loss:  -0.9822: : 6144000it [01:05, 93207.15it/s]                                                   \n",
      "loss:  -0.9944: : 6144000it [22:26, 4562.84it/s]                                                    \n",
      "loss:  -0.9850: : 6144000it [01:08, 89696.07it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93768.20it/s]                                                   \n",
      "loss:  -0.9778: : 6144000it [01:06, 92772.87it/s]                                                   \n",
      "loss:  -0.9950: : 6144000it [22:26, 4563.81it/s]                                                    \n",
      "loss:  -0.9887: : 6144000it [01:08, 89845.62it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93805.58it/s]                                                   \n",
      "loss:  -0.9819: : 6144000it [01:06, 92827.85it/s]                                                   \n",
      "loss:  -0.9942: : 6144000it [22:30, 4549.74it/s]                                                    \n",
      "loss:  -0.9855: : 6144000it [01:08, 89628.85it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93270.49it/s]                                                   \n",
      "loss:  -0.9834: : 6144000it [01:05, 93216.50it/s]                                                   \n",
      "loss:  -0.9949: : 6144000it [22:30, 4550.40it/s]                                                    \n",
      "loss:  -0.9853: : 6144000it [01:08, 89949.19it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93352.21it/s]                                                   \n",
      "loss:  -0.9826: : 6144000it [01:05, 93092.16it/s]                                                   \n",
      "loss:  -0.9945: : 6144000it [22:30, 4550.21it/s]                                                    \n",
      "loss:  -0.9889: : 6144000it [01:08, 89789.88it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 91811.16it/s]                                                   \n",
      "loss:  -0.8838: : 6144000it [01:06, 92234.52it/s]                                                   \n",
      "loss:  -0.9949: : 6144000it [22:30, 4549.79it/s]                                                    \n",
      "loss:  -0.9803: : 6144000it [01:08, 89684.73it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93548.28it/s]                                                   \n",
      "loss:  -0.9835: : 6144000it [01:07, 91140.72it/s]                                                   \n",
      "loss:  -0.9948: : 6144000it [22:30, 4549.94it/s]                                                    \n",
      "loss:  -0.9886: : 6144000it [01:08, 89578.17it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93756.58it/s]                                                   \n",
      "loss:  -0.8853: : 6144000it [01:06, 92939.92it/s]                                                   \n",
      "loss:  -0.9052: : 6144000it [22:30, 4547.74it/s]                                                    \n",
      "loss:  -0.9881: : 6144000it [01:08, 89356.20it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:05, 93429.57it/s]                                                   \n",
      "loss:  -0.9831: : 6144000it [01:06, 92825.15it/s]                                                   \n",
      "loss:  -0.9939: : 6144000it [22:30, 4548.73it/s]                                                    \n",
      "loss:  -0.9778: : 6144000it [01:08, 89861.08it/s]                                                   \n",
      "loss:  -0.8452: : 6144000it [01:05, 93759.74it/s]                                                   \n",
      "loss:  -0.9829: : 6144000it [01:05, 93095.89it/s]                                                   \n",
      "loss:  -0.9944: : 6144000it [22:29, 4551.11it/s]                                                    \n",
      "loss:  -0.8921: : 6144000it [01:08, 89654.11it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93580.58it/s]                                                   \n",
      "loss:  -0.9825: : 6144000it [01:05, 93364.30it/s]                                                   \n",
      "loss:  -0.9911: : 6144000it [22:29, 4551.49it/s]                                                    \n",
      "loss:  -0.8871: : 6144000it [01:08, 89870.77it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93836.69it/s]                                                   \n",
      "loss:  -0.9826: : 6144000it [01:05, 93134.43it/s]                                                   \n",
      "loss:  -0.9950: : 6144000it [22:29, 4552.72it/s]                                                    \n",
      "loss:  -0.9867: : 6144000it [01:09, 88611.56it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:06, 92086.13it/s]                                                   \n",
      "loss:  -0.8852: : 6144000it [01:05, 93186.15it/s]                                                   \n",
      "loss:  -0.9949: : 6144000it [22:29, 4553.22it/s]                                                    \n",
      "loss:  -0.9876: : 6144000it [01:08, 89599.85it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93624.97it/s]                                                   \n",
      "loss:  -0.8871: : 6144000it [01:05, 93137.30it/s]                                                   \n",
      "loss:  -0.9952: : 6144000it [22:29, 4553.25it/s]                                                    \n",
      "loss:  -0.9796: : 6144000it [01:08, 89994.70it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:05, 93842.83it/s]                                                   \n",
      "loss:  -0.9839: : 6144000it [01:05, 93215.69it/s]                                                   \n",
      "loss:  -0.9878: : 6144000it [22:29, 4551.76it/s]                                                    \n",
      "loss:  -0.9822: : 6144000it [01:10, 86745.78it/s]                                                   \n",
      "loss:  -0.9253: : 6144000it [01:04, 95178.42it/s]                                                   \n",
      "loss:  -0.9829: : 6144000it [01:05, 94141.17it/s]                                                   \n",
      "loss:  -0.9948: : 6144000it [25:32, 4008.41it/s]                                                    \n",
      "loss:  -0.9902: : 6144000it [01:09, 87788.43it/s]                                                   \n",
      "loss:  -0.9252: : 6144000it [01:04, 94832.32it/s]                                                   \n",
      "loss:  -0.9810: : 6144000it [01:05, 93401.11it/s]                                                   \n",
      "loss:  -0.9927:  83%|█████████████████████████████      | 4976640/6000000 [20:15<02:36, 6526.83it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    n_class = 10\n",
    "\n",
    "    linear = nn.Sequential(\n",
    "        nn.Flatten(),                  # Flatten images from a 2D matrix to 1D => (28x28) => (784,)\n",
    "        nn.LazyLinear(n_class),        # Simple (a x + b) layer that will learn `a` and `b`\n",
    "        nn.Softmax(dim=1)              # Normalization so result wil be between [0, 1]\n",
    "    )\n",
    "\n",
    "    deepmodel = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.LazyLinear(128),    # r1 = batch * w1 + b1\n",
    "        nn.ReLU(),             # r2 = max(r1, 0)\n",
    "        nn.LazyLinear(64),     # r3 = r3 * w3 + b3\n",
    "        nn.ReLU(),             # r4 = max(r3, 0)\n",
    "        nn.LazyLinear(n_class),# r5 = r4 * w4 + b4\n",
    "        nn.Softmax(dim=1)  \n",
    "    )\n",
    "\n",
    "    conv_model = nn.Sequential(\n",
    "        # ext_nn.MaskLayer((28, 28)),\n",
    "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(576 * 64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_class),\n",
    "        nn.Softmax(dim=1),\n",
    "    )\n",
    "\n",
    "    linear_stn = nn.Sequential(\n",
    "        STN(),\n",
    "        nn.Flatten(),                  # Flatten images from a 2D matrix to 1D => (28x28) => (784,)\n",
    "        nn.Linear(784, n_class),        # Simple (a x + b) layer that will learn `a` and `b`\n",
    "        nn.Softmax(dim=1)              # Normalization so result wil be between [0, 1]\n",
    "    )\n",
    "    \n",
    "    models = [\n",
    "        ('linear', linear),\n",
    "        ('deepmodel', deepmodel),\n",
    "        ('conv_model', conv_model),\n",
    "        ('linear_stn', linear_stn),\n",
    "    ]\n",
    "    \n",
    "    for k, v in models:\n",
    "        trainfast(k, v, epoch=100, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d709b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss_curve('time', 'train_loss', 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c237de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "metric encoding field is specified without a type; the type cannot be inferred because it does not match any column in the data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\vegalite\\v4\\api.py:2020\u001b[0m, in \u001b[0;36mChart.to_dict\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2018\u001b[0m     copy\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mInlineData(values\u001b[38;5;241m=\u001b[39m[{}])\n\u001b[0;32m   2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Chart, copy)\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 2020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\vegalite\\v4\\api.py:384\u001b[0m, in \u001b[0;36mTopLevelMixin.to_dict\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m context\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     dct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(TopLevelMixin, copy)\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m jsonschema\u001b[38;5;241m.\u001b[39mValidationError:\n\u001b[0;32m    386\u001b[0m     dct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:326\u001b[0m, in \u001b[0;36mSchemaBase.to_dict\u001b[1;34m(self, validate, ignore, context)\u001b[0m\n\u001b[0;32m    324\u001b[0m     result \u001b[38;5;241m=\u001b[39m _todict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m], validate\u001b[38;5;241m=\u001b[39msub_validate, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args:\n\u001b[1;32m--> 326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_todict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_validate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instance has both a value and properties : \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot serialize to dict\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m    335\u001b[0m     )\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:60\u001b[0m, in \u001b[0;36m_todict\u001b[1;34m(obj, validate, context)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_todict(v, validate, context) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     61\u001b[0m         k: _todict(v, validate, context)\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Undefined\n\u001b[0;32m     64\u001b[0m     }\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:61\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_todict(v, validate, context) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 61\u001b[0m         k: \u001b[43m_todict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Undefined\n\u001b[0;32m     64\u001b[0m     }\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:56\u001b[0m, in \u001b[0;36m_todict\u001b[1;34m(obj, validate, context)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"Convert an object to a dict representation.\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, SchemaBase):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_todict(v, validate, context) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:326\u001b[0m, in \u001b[0;36mSchemaBase.to_dict\u001b[1;34m(self, validate, ignore, context)\u001b[0m\n\u001b[0;32m    324\u001b[0m     result \u001b[38;5;241m=\u001b[39m _todict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m], validate\u001b[38;5;241m=\u001b[39msub_validate, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args:\n\u001b[1;32m--> 326\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_todict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_validate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instance has both a value and properties : \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot serialize to dict\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m    335\u001b[0m     )\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:60\u001b[0m, in \u001b[0;36m_todict\u001b[1;34m(obj, validate, context)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_todict(v, validate, context) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     61\u001b[0m         k: _todict(v, validate, context)\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Undefined\n\u001b[0;32m     64\u001b[0m     }\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:61\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_todict(v, validate, context) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 61\u001b[0m         k: \u001b[43m_todict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Undefined\n\u001b[0;32m     64\u001b[0m     }\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\utils\\schemapi.py:56\u001b[0m, in \u001b[0;36m_todict\u001b[1;34m(obj, validate, context)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"Convert an object to a dict representation.\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, SchemaBase):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_todict(v, validate, context) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n",
      "File \u001b[1;32mE:\\Anaconda\\lib\\site-packages\\altair\\vegalite\\v4\\schema\\channels.py:40\u001b[0m, in \u001b[0;36mFieldChannelMixin.to_dict\u001b[1;34m(self, validate, ignore, context)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (type_in_shorthand \u001b[38;5;129;01mor\u001b[39;00m type_defined_explicitly):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m---> 40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m encoding field is specified without a type; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe type cannot be inferred because it does not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch any column in the data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shorthand))\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m encoding field is specified without a type; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe type cannot be automatically inferred because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe data is not specified as a pandas.DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shorthand))\n",
      "\u001b[1;31mValueError\u001b[0m: metric encoding field is specified without a type; the type cannot be inferred because it does not match any column in the data."
     ]
    },
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_train_test_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38784ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newmodel(depth):\n",
    "    base = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(784, 128)\n",
    "    )\n",
    "    \n",
    "    mid = []\n",
    "    for i in range(depth * 4):\n",
    "        mid.extend([nn.ReLU(), nn.Linear(128, 128)])\n",
    "    mid = nn.Sequential(*mid)\n",
    "    \n",
    "    end = nn.Sequential(\n",
    "        nn.ReLU(),    \n",
    "        nn.Linear(128, 10),     \n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "    return nn.Sequential(\n",
    "        base,\n",
    "        mid,\n",
    "        end,\n",
    "    )\n",
    "\n",
    "def move_weights(destination_model, source_model):\n",
    "    dest = destination_model.state_dict()\n",
    "    src = source_model.state_dict()\n",
    "    \n",
    "    for k, v in src.items():\n",
    "        if k in dest:\n",
    "            dest[k] = v\n",
    "            \n",
    "    destination_model.load_state_dict(dest)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d83ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m previous_model \u001b[38;5;241m=\u001b[39m \u001b[43mnewmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [13], line 2\u001b[0m, in \u001b[0;36mnewmodel\u001b[1;34m(depth)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewmodel\u001b[39m(depth):\n\u001b[1;32m----> 2\u001b[0m     base \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      3\u001b[0m         nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m      4\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      7\u001b[0m     mid \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "previous_model = newmodel(1)\n",
    "\n",
    "for i in range(5):\n",
    "    print(i + 1)\n",
    "    \n",
    "    next_model = newmodel(i + 1)\n",
    "    move_weights(next_model, previous_model)\n",
    "\n",
    "    loss_tracker[f'model_{i + 1}'] = []\n",
    "    trainfast(f'model_{i + 1}', next_model, lr=0.1, epoch=50)\n",
    "    show_loss_curve()\n",
    "    \n",
    "    previous_model = next_model\n",
    "    \n",
    "show_loss_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281b2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
